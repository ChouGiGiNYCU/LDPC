{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42a24af9-ce27-42f9-a8b0-1e7b7218aa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28e95ceb-8764-4f60-8518-319a17516dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cpu.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1a6e787-aa2c-4c93-8808-4ec4342c0fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parity_check_matrix_rowsize :  4\n",
      "parity_check_matrix_colsize :  15\n",
      "hidden_layer_node :  33\n"
     ]
    }
   ],
   "source": [
    "parity_check_matrix = [\n",
    "    [1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1],\n",
    "    [0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1],\n",
    "    [0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1],\n",
    "    [0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1]\n",
    "]\n",
    "parity_check_matrix_colsize = len(parity_check_matrix[0])\n",
    "parity_check_matrix_rowsize = len(parity_check_matrix)\n",
    "# first hidden layer node number count 1 number in parity_matrix\n",
    "hidden_layer_node = sum(sum(row) for row in parity_check_matrix)\n",
    "\n",
    "\n",
    "print(\"parity_check_matrix_rowsize : \",parity_check_matrix_rowsize)\n",
    "print(\"parity_check_matrix_colsize : \",parity_check_matrix_colsize)\n",
    "print(\"hidden_layer_node : \",hidden_layer_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f6c1a60-b2fd-4274-9dc1-5445f270d450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_row_degree :  9\n"
     ]
    }
   ],
   "source": [
    "CN2VN_pos = []\n",
    "max_row_degree =0 \n",
    "for r in range(0,len(parity_check_matrix)):\n",
    "    tmp=[]\n",
    "    for c in range(0,len(parity_check_matrix[0])):\n",
    "        if parity_check_matrix[r][c]==1:\n",
    "            tmp.append(c)\n",
    "    max_row_degree = max(max_row_degree,len(tmp))\n",
    "    CN2VN_pos.append(tmp)\n",
    "# print(\"CN2VN_pos:\")\n",
    "# CN2VN_pos\n",
    "print(\"max_row_degree : \",max_row_degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa7ff099-0943-46f8-8e19-cd803c9cc5b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_col_degree :  4\n"
     ]
    }
   ],
   "source": [
    "VN2CN=[]\n",
    "max_col_degree=0\n",
    "for c in range(0,len(parity_check_matrix[0])):\n",
    "    tmp =[]\n",
    "    for r in range(0,len(parity_check_matrix)):\n",
    "        if parity_check_matrix[r][c]==1:\n",
    "            tmp.append(r)\n",
    "    VN2CN.append(tmp)\n",
    "    max_col_degree = max(max_col_degree,len(tmp))\n",
    "# print(\"VN2CN : \")\n",
    "# VN2CN\n",
    "print(\"max_col_degree : \",max_col_degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfc3d62-119f-43e3-aaf1-c8bac090bcdb",
   "metadata": {},
   "source": [
    "### Write parity_check_matrix to txt file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10da7bab-cd3c-4d4e-9aed-9e501bd57590",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"BCH_15_11.txt\", \"w\") as txt_file:\n",
    "    txt_file.write(\"{} {} \\n\".format(parity_check_matrix_colsize ,parity_check_matrix_rowsize))\n",
    "    txt_file.write(\"{} {} \\n\".format(max_col_degree,max_row_degree))\n",
    "    for i in range(0,len(VN2CN)):\n",
    "        txt_file.write(\"{} \".format(len(VN2CN[i])))\n",
    "    txt_file.write(\"\\n\")\n",
    "    for i in range(0,len(CN2VN_pos)):\n",
    "        txt_file.write(\"{} \".format(len(CN2VN_pos[i])))\n",
    "    txt_file.write(\"\\n\")\n",
    "    for i in range(0,len(VN2CN)):\n",
    "        for j in range(0,len(VN2CN[i])):\n",
    "            txt_file.write(\"{} \".format(VN2CN[i][j]+1))\n",
    "        if len(VN2CN[i])!=max_col_degree:\n",
    "            for j in range(0,max_col_degree-len(VN2CN[i])):\n",
    "                txt_file.write(\"{} \".format(0))\n",
    "        txt_file.write(\"\\n\")\n",
    "    for i in range(0,len(CN2VN_pos)):\n",
    "        for j in range(0,len(CN2VN_pos[i])):\n",
    "            txt_file.write(\"{} \".format(CN2VN_pos[i][j]+1))\n",
    "        if len(CN2VN_pos[i])!=max_row_degree:\n",
    "            for j in range(0,max_row_degree-len(CN2VN_pos[i])):\n",
    "                txt_file.write(\"{} \".format(0))\n",
    "        txt_file.write(\"\\n\")\n",
    "    txt_file.write(\"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "600a4b79-1c4e-4b5d-86dd-cb796c5420f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - [0]\n",
      "1 - [9]\n",
      "2 - [17]\n",
      "3 - [25]\n",
      "4 - [1, 18, 26]\n",
      "5 - [10, 19, 27]\n",
      "6 - [28]\n",
      "7 - [2, 11]\n",
      "8 - [12, 20]\n",
      "9 - [3, 21, 29]\n",
      "10 - [4, 30]\n",
      "11 - [5, 13, 31]\n",
      "12 - [6, 14, 22]\n",
      "13 - [7, 15, 23]\n",
      "14 - [8, 16, 24, 32]\n",
      "(0, 0) - 0\n",
      "(1, 1) - 1\n",
      "(2, 2) - 2\n",
      "(3, 3) - 3\n",
      "(4, 0) - 4\n",
      "(4, 2) - 5\n",
      "(4, 3) - 6\n",
      "(5, 1) - 7\n",
      "(5, 2) - 8\n",
      "(5, 3) - 9\n",
      "(6, 3) - 10\n",
      "(7, 0) - 11\n",
      "(7, 1) - 12\n",
      "(8, 1) - 13\n",
      "(8, 2) - 14\n",
      "(9, 0) - 15\n",
      "(9, 2) - 16\n",
      "(9, 3) - 17\n",
      "(10, 0) - 18\n",
      "(10, 3) - 19\n",
      "(11, 0) - 20\n",
      "(11, 1) - 21\n",
      "(11, 3) - 22\n",
      "(12, 0) - 23\n",
      "(12, 1) - 24\n",
      "(12, 2) - 25\n",
      "(13, 0) - 26\n",
      "(13, 1) - 27\n",
      "(13, 2) - 28\n",
      "(14, 0) - 29\n",
      "(14, 1) - 30\n",
      "(14, 2) - 31\n",
      "(14, 3) - 32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: [0],\n",
       " 1: [9],\n",
       " 2: [17],\n",
       " 3: [25],\n",
       " 4: [1, 18, 26],\n",
       " 5: [10, 19, 27],\n",
       " 6: [28],\n",
       " 7: [2, 11],\n",
       " 8: [12, 20],\n",
       " 9: [3, 21, 29],\n",
       " 10: [4, 30],\n",
       " 11: [5, 13, 31],\n",
       " 12: [6, 14, 22],\n",
       " 13: [7, 15, 23],\n",
       " 14: [8, 16, 24, 32]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "connect_first_layer = []\n",
    "dict_update_cn = {}\n",
    "dict_update_cn_arr = defaultdict(list)\n",
    "dict_vn_pos = defaultdict(list)\n",
    "# for first layer mask\n",
    "for CN in range(0,len(CN2VN_pos)):\n",
    "    for c in range(0,len(CN2VN_pos[CN])):\n",
    "        tmp = CN2VN_pos[CN][0:c] + CN2VN_pos[CN][c+1:len(CN2VN_pos[CN])]\n",
    "        connect_first_layer.append(tmp)\n",
    "        dict_update_cn[tuple(tmp)]=(CN2VN_pos[CN][c],CN) # [connect to VNs] - update(CN->CN2VN_pos[r][c]) (VN,CN)\n",
    "        dict_update_cn_arr[(CN2VN_pos[CN][c],CN)].append(tuple(tmp))\n",
    "        # print(dict_update_cn[tuple(tmp)], \"-\",tuple(tmp))\n",
    "        \n",
    "\n",
    "# for second layer mask\n",
    "# save neural network node position ([connect to VNs]- node_position)\n",
    "# print(\"second layer : \")\n",
    "dict_connect_first_layer ={}\n",
    "for idx,arr in enumerate(connect_first_layer):\n",
    "    dict_connect_first_layer[tuple(arr)]=idx \n",
    "    # print(idx,\" - \",tuple(arr))\n",
    "\n",
    "for key in sorted(dict_update_cn_arr.keys(),key=lambda x:x[1]):\n",
    "    VN = key[0]\n",
    "    for arr in dict_update_cn_arr[key]:\n",
    "        dict_vn_pos[VN].append(dict_connect_first_layer[arr])\n",
    "# sort \n",
    "sorted_keys = sorted(dict_vn_pos.keys(),key=lambda x:x)\n",
    "dict_vn_pos_sort = {key: dict_vn_pos[key] for key in sorted_keys}\n",
    "dict_vn_pos=dict_vn_pos_sort.copy()\n",
    "# show update key with node\n",
    "for key in dict_vn_pos.keys():\n",
    "    print(key,\"-\",dict_vn_pos[key])\n",
    "\n",
    "#constuct third mask of VN->CN update (record VN->CN node postion in neural network)\n",
    "dict_vn_to_cn_node = defaultdict(list)\n",
    "idx=0\n",
    "for VN in dict_vn_pos.keys():\n",
    "    for CN in VN2CN[VN]:\n",
    "        dict_vn_to_cn_node[(VN,CN)] = idx\n",
    "        idx+=1\n",
    "for key in dict_vn_to_cn_node.keys():\n",
    "    print(key,\"-\",dict_vn_to_cn_node[key])\n",
    "\n",
    "# construct fourth mask of  CN ->VN + channel LLR = output\n",
    "dict_cn_to_output = defaultdict(list)\n",
    "idx=0\n",
    "for CN in range(0,len(CN2VN_pos)):\n",
    "    for VN in CN2VN_pos[CN]:\n",
    "        dict_cn_to_output[VN].append(idx)\n",
    "        idx+=1\n",
    "# sort \n",
    "sorted_keys = sorted(dict_cn_to_output.keys(),key=lambda x:x)\n",
    "dict_cn_to_output_sort = {key: dict_cn_to_output[key] for key in sorted_keys}\n",
    "dict_cn_to_output=dict_cn_to_output_sort.copy()\n",
    "dict_cn_to_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b933ec-1712-4328-ae29-1da20e179059",
   "metadata": {},
   "source": [
    "### Construct every layer mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50766b41-a119-4de2-9880-9d352b135dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_mask rowsize is : 33  , colsize :  15\n"
     ]
    }
   ],
   "source": [
    "# construct first layer mask (CN update)\n",
    "first_layer_mask = [[False]*parity_check_matrix_colsize for i in range(0,hidden_layer_node)]\n",
    "\n",
    "print(\"layer_mask rowsize is :\",len(first_layer_mask),\" , colsize : \",len(first_layer_mask[0]))\n",
    "\n",
    "for i in range(0,len(connect_first_layer)):\n",
    "    for num in connect_first_layer[i]:\n",
    "        first_layer_mask[i][num]=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "194238cd-b49e-47da-8fd4-c310ab724719",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neourn : 0 - [] | VN 0 -> CN 0\n",
      "neourn : 1 - [] | VN 1 -> CN 1\n",
      "neourn : 2 - [] | VN 2 -> CN 2\n",
      "neourn : 3 - [] | VN 3 -> CN 3\n",
      "neourn : 4 - [18, 26] | VN 4 -> CN 0\n",
      "neourn : 5 - [1, 26] | VN 4 -> CN 2\n",
      "neourn : 6 - [1, 18] | VN 4 -> CN 3\n",
      "neourn : 7 - [19, 27] | VN 5 -> CN 1\n",
      "neourn : 8 - [10, 27] | VN 5 -> CN 2\n",
      "neourn : 9 - [10, 19] | VN 5 -> CN 3\n",
      "neourn : 10 - [] | VN 6 -> CN 3\n",
      "neourn : 11 - [11] | VN 7 -> CN 0\n",
      "neourn : 12 - [2] | VN 7 -> CN 1\n",
      "neourn : 13 - [20] | VN 8 -> CN 1\n",
      "neourn : 14 - [12] | VN 8 -> CN 2\n",
      "neourn : 15 - [21, 29] | VN 9 -> CN 0\n",
      "neourn : 16 - [3, 29] | VN 9 -> CN 2\n",
      "neourn : 17 - [3, 21] | VN 9 -> CN 3\n",
      "neourn : 18 - [30] | VN 10 -> CN 0\n",
      "neourn : 19 - [4] | VN 10 -> CN 3\n",
      "neourn : 20 - [13, 31] | VN 11 -> CN 0\n",
      "neourn : 21 - [5, 31] | VN 11 -> CN 1\n",
      "neourn : 22 - [5, 13] | VN 11 -> CN 3\n",
      "neourn : 23 - [14, 22] | VN 12 -> CN 0\n",
      "neourn : 24 - [6, 22] | VN 12 -> CN 1\n",
      "neourn : 25 - [6, 14] | VN 12 -> CN 2\n",
      "neourn : 26 - [15, 23] | VN 13 -> CN 0\n",
      "neourn : 27 - [7, 23] | VN 13 -> CN 1\n",
      "neourn : 28 - [7, 15] | VN 13 -> CN 2\n",
      "neourn : 29 - [16, 24, 32] | VN 14 -> CN 0\n",
      "neourn : 30 - [8, 24, 32] | VN 14 -> CN 1\n",
      "neourn : 31 - [8, 16, 32] | VN 14 -> CN 2\n",
      "neourn : 32 - [8, 16, 24] | VN 14 -> CN 3\n"
     ]
    }
   ],
   "source": [
    "# construct CN2VN mask - second layer connect table matrix(mask)\n",
    "CN2VN_mask_VNUpdate = [[False]*hidden_layer_node for i in range(0,hidden_layer_node)]\n",
    "idx=0\n",
    "tmp=[]\n",
    "for VN in dict_vn_pos.keys():\n",
    "    if(len(dict_vn_pos[VN])==1):\n",
    "        idx+=1\n",
    "        tmp.append([VN2CN[VN][0],VN])\n",
    "        continue\n",
    "    for i,node1 in enumerate(dict_vn_pos[VN]):\n",
    "        for node2 in dict_vn_pos[VN]:\n",
    "            if node1!=node2:\n",
    "                CN2VN_mask_VNUpdate[idx][node2]=True\n",
    "        idx+=1\n",
    "        tmp.append([VN2CN[VN][i],VN]) # CN,VN\n",
    "true_indices_per_row = [[idx for idx, val in enumerate(row) if val] for row in CN2VN_mask_VNUpdate]\n",
    "for idx,arr in enumerate(true_indices_per_row):\n",
    "    print(\"neourn : {} - {} | VN {} -> CN {}\".format(idx, arr, tmp[idx][1], tmp[idx][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0abb365-bd81-4c9e-b6f6-0218e1497bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx :  33\n",
      "neourn : 0 - [4, 11, 15, 18, 20, 23, 26, 29] | CN 0 -> VN 0\n",
      "neourn : 1 - [0, 11, 15, 18, 20, 23, 26, 29] | CN 0 -> VN 4\n",
      "neourn : 2 - [0, 4, 15, 18, 20, 23, 26, 29] | CN 0 -> VN 7\n",
      "neourn : 3 - [0, 4, 11, 18, 20, 23, 26, 29] | CN 0 -> VN 9\n",
      "neourn : 4 - [0, 4, 11, 15, 20, 23, 26, 29] | CN 0 -> VN 10\n",
      "neourn : 5 - [0, 4, 11, 15, 18, 23, 26, 29] | CN 0 -> VN 11\n",
      "neourn : 6 - [0, 4, 11, 15, 18, 20, 26, 29] | CN 0 -> VN 12\n",
      "neourn : 7 - [0, 4, 11, 15, 18, 20, 23, 29] | CN 0 -> VN 13\n",
      "neourn : 8 - [0, 4, 11, 15, 18, 20, 23, 26] | CN 0 -> VN 14\n",
      "neourn : 9 - [7, 12, 13, 21, 24, 27, 30] | CN 1 -> VN 1\n",
      "neourn : 10 - [1, 12, 13, 21, 24, 27, 30] | CN 1 -> VN 5\n",
      "neourn : 11 - [1, 7, 13, 21, 24, 27, 30] | CN 1 -> VN 7\n",
      "neourn : 12 - [1, 7, 12, 21, 24, 27, 30] | CN 1 -> VN 8\n",
      "neourn : 13 - [1, 7, 12, 13, 24, 27, 30] | CN 1 -> VN 11\n",
      "neourn : 14 - [1, 7, 12, 13, 21, 27, 30] | CN 1 -> VN 12\n",
      "neourn : 15 - [1, 7, 12, 13, 21, 24, 30] | CN 1 -> VN 13\n",
      "neourn : 16 - [1, 7, 12, 13, 21, 24, 27] | CN 1 -> VN 14\n",
      "neourn : 17 - [5, 8, 14, 16, 25, 28, 31] | CN 2 -> VN 2\n",
      "neourn : 18 - [2, 8, 14, 16, 25, 28, 31] | CN 2 -> VN 4\n",
      "neourn : 19 - [2, 5, 14, 16, 25, 28, 31] | CN 2 -> VN 5\n",
      "neourn : 20 - [2, 5, 8, 16, 25, 28, 31] | CN 2 -> VN 8\n",
      "neourn : 21 - [2, 5, 8, 14, 25, 28, 31] | CN 2 -> VN 9\n",
      "neourn : 22 - [2, 5, 8, 14, 16, 28, 31] | CN 2 -> VN 12\n",
      "neourn : 23 - [2, 5, 8, 14, 16, 25, 31] | CN 2 -> VN 13\n",
      "neourn : 24 - [2, 5, 8, 14, 16, 25, 28] | CN 2 -> VN 14\n",
      "neourn : 25 - [6, 9, 10, 17, 19, 22, 32] | CN 3 -> VN 3\n",
      "neourn : 26 - [3, 9, 10, 17, 19, 22, 32] | CN 3 -> VN 4\n",
      "neourn : 27 - [3, 6, 10, 17, 19, 22, 32] | CN 3 -> VN 5\n",
      "neourn : 28 - [3, 6, 9, 17, 19, 22, 32] | CN 3 -> VN 6\n",
      "neourn : 29 - [3, 6, 9, 10, 19, 22, 32] | CN 3 -> VN 9\n",
      "neourn : 30 - [3, 6, 9, 10, 17, 22, 32] | CN 3 -> VN 10\n",
      "neourn : 31 - [3, 6, 9, 10, 17, 19, 32] | CN 3 -> VN 11\n",
      "neourn : 32 - [3, 6, 9, 10, 17, 19, 22] | CN 3 -> VN 14\n"
     ]
    }
   ],
   "source": [
    "#construct third mask(VN->CN)\n",
    "VN2CN_mask_CNUpdate = [[False]*hidden_layer_node for i in range(0,hidden_layer_node)]\n",
    "# 照著cn0->vn1 -> cn0->vn2 ...順序\n",
    "idx=0\n",
    "CN_2_VN_record=[]\n",
    "for CN in range(0,parity_check_matrix_rowsize):\n",
    "    #  CN->VN1 update\n",
    "    for VN1 in  CN2VN_pos[CN]: \n",
    "        for VN2 in CN2VN_pos[CN]:\n",
    "            if VN1!=VN2:\n",
    "                # print(dict_vn_to_cn_node[(VN2,CN)])\n",
    "                VN2CN_mask_CNUpdate[idx][dict_vn_to_cn_node[(VN2,CN)]]=True\n",
    "        idx+=1\n",
    "        CN_2_VN_record.append([CN,VN1])\n",
    "print(\"idx : \",idx)\n",
    "true_indices_per_row = [[idx for idx, val in enumerate(row) if val] for row in VN2CN_mask_CNUpdate]\n",
    "for idx,arr in enumerate(true_indices_per_row):\n",
    "    # print(\"node : \",idx,\" - \",arr,\"| CN\",CN_2_VN_record[idx][0],\"->\",\"VN\",CN_2_VN_record[idx][1])\n",
    "    print(\"neourn : {} - {} | CN {} -> VN {}\".format(idx, arr, CN_2_VN_record[idx][0], CN_2_VN_record[idx][1]))\n",
    "# print(VN2CN_mask_CNUpdate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adbf5103-178d-461c-8fd5-6d3dc70d1e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neourn : 0 - [0] \n",
      "neourn : 1 - [9] \n",
      "neourn : 2 - [17] \n",
      "neourn : 3 - [25] \n",
      "neourn : 4 - [1, 18, 26] \n",
      "neourn : 5 - [10, 19, 27] \n",
      "neourn : 6 - [28] \n",
      "neourn : 7 - [2, 11] \n",
      "neourn : 8 - [12, 20] \n",
      "neourn : 9 - [3, 21, 29] \n",
      "neourn : 10 - [4, 30] \n",
      "neourn : 11 - [5, 13, 31] \n",
      "neourn : 12 - [6, 14, 22] \n",
      "neourn : 13 - [7, 15, 23] \n",
      "neourn : 14 - [8, 16, 24, 32] \n"
     ]
    }
   ],
   "source": [
    "# construct fouth final CN update to result\n",
    "CN2VN_mask_output = [[False]*hidden_layer_node for i in range(0,parity_check_matrix_colsize)]\n",
    "for VN in range(0,len(dict_cn_to_output)):\n",
    "    for node in dict_cn_to_output[VN]:\n",
    "        CN2VN_mask_output[VN][node] = True\n",
    "    # print(CN2VN_mask_output[VN])\n",
    "    \n",
    "true_indices_per_row = [[idx for idx, val in enumerate(row) if val] for row in CN2VN_mask_output]\n",
    "for idx,arr in enumerate(true_indices_per_row):\n",
    "    print(\"neourn : {} - {} \".format(idx, arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2097c69-c1c2-4808-9403-0948e60626f5",
   "metadata": {},
   "source": [
    "### Construct every layer bias(only VN update need bias(channel LLR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e112fb32-8e07-4bee-b0a2-18915f569b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 神經網路裡面的bias=[LLR1 ... LLR_VN]*matrix\n",
    "dot_bias_matrix=torch.zeros(parity_check_matrix_colsize ,hidden_layer_node)\n",
    "idx=0\n",
    "for i in range(0,len(VN2CN)):\n",
    "    for j in range(0,len(VN2CN[i])):\n",
    "        dot_bias_matrix[i][idx]=1;\n",
    "        idx = idx + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88948f98-2df2-44e4-99b2-79da56f3015e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights of fc0: Parameter containing:\n",
      "tensor([[0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1.],\n",
      "        [1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1.],\n",
      "        [1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1.],\n",
      "        [1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1.],\n",
      "        [0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1.],\n",
      "        [0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1.],\n",
      "        [0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1.],\n",
      "        [0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1.],\n",
      "        [0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0.],\n",
      "        [0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1.],\n",
      "        [0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1.],\n",
      "        [0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1.],\n",
      "        [0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1.],\n",
      "        [0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1.],\n",
      "        [0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1.],\n",
      "        [0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0.],\n",
      "        [0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0.]])\n",
      "Initial weights of fc1: Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True)\n",
      "Initial weights of fc2: Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "Initial weights of fc3: Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True)\n",
      "Initial weights of fc4: Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "Initial weights of fc5: Parameter containing:\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 定义模型参数\n",
    "input_size = parity_check_matrix_colsize\n",
    "hidden_layer_node = hidden_layer_node\n",
    "output_size = parity_check_matrix_colsize\n",
    "# VN update Layer\n",
    "class VnUpdateLayer(nn.Module): # Apply Add\n",
    "    def __init__(self, in_features, out_features,layer_mask):\n",
    "        super(VnUpdateLayer, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.bias = nn.Parameter(torch.Tensor(out_features))\n",
    "        self.layer_mask = torch.tensor(layer_mask, dtype=torch.float32)  # Convert to tensor\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        # nn.init.uniform_(self.weight,-1,1) # 初始化權重 range(-1,1)\n",
    "        nn.init.ones_(self.weight) # 初始化權重 range(-1,1)\n",
    "        self.weight.requires_grad=True\n",
    "        nn.init.zeros_(self.bias)\n",
    "        self.bias.requires_grad = False\n",
    "        \n",
    "        if self.layer_mask is not None:\n",
    "            self.weight.data *= self.layer_mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.matmul(x, self.weight.t())\n",
    "    \n",
    "    def apply_sparse_mask(self):\n",
    "         # 将稀疏掩码为零的权重的梯度设为零\n",
    "        if self.weight.grad is not None:\n",
    "            self.weight.grad.data *= self.layer_mask\n",
    "\n",
    "# CN update Layer (Dont need update paramter or weight)\n",
    "class CnUpdateLayer(nn.Module): # apply dot\n",
    "    def __init__(self, in_features, out_features,layer_mask,first=False):\n",
    "        super(CnUpdateLayer, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.bias = nn.Parameter(torch.Tensor(out_features))\n",
    "        self.out_features = out_features\n",
    "        self.layer_mask = torch.tensor(layer_mask, dtype=torch.float32)  # Convert to tensor\n",
    "        self.first = first\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "       \n",
    "        nn.init.ones_(self.weight) # 初始化權重 \n",
    "        self.weight.requires_grad=False\n",
    "        nn.init.zeros_(self.bias)\n",
    "        self.bias.requires_grad = False\n",
    "        \n",
    "        if self.layer_mask is not None:\n",
    "            self.weight.data *= self.layer_mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 应用掩码并处理每个输出神经元\n",
    "        batch_size = x.size(0)\n",
    "        output = torch.zeros(batch_size, self.out_features, device=x.device)\n",
    "        for i in range(self.out_features):\n",
    "            # 应用每个输出神经元的掩码\n",
    "            mask = self.layer_mask[i]\n",
    "            x_masked = x * mask  # 掩码应用于输入数据\n",
    "            # 将零值替换为一，以避免影响乘积\n",
    "            x_nonzero = torch.where(x_masked != 0, x_masked, torch.ones_like(x_masked))\n",
    "            # 计算掩码后数据的乘积\n",
    "            product = torch.prod(x_nonzero, dim=1)\n",
    "            # 检查全零行，将这些行的乘积结果置为1，避免影响后续的双曲正切反函数\n",
    "            non_zero_prod = torch.where((x_masked != 0).any(dim=1), product, torch.zeros_like(product))\n",
    "            # 计算双曲正切反函数并乘以2\n",
    "            output[:, i] = non_zero_prod\n",
    "        \n",
    "        return output\n",
    "    def apply_sparse_mask(self):\n",
    "        pass\n",
    "         # 将稀疏掩码为零的权重的梯度设为零\n",
    "        # if self.weight.grad is not None:\n",
    "        #     self.weight.grad.data *= self.layer_mask\n",
    "            \n",
    "class SparseBPNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size,first_layer_mask,CN2VN_mask_VNUpdate,VN2CN_mask_CNUpdate,CN2VN_mask_output,dot_bias_matrix):\n",
    "        super(SparseBPNeuralNetwork, self).__init__()\n",
    "        self.fc0 = CnUpdateLayer(input_size, hidden_size,first_layer_mask,first=True) # CN update\n",
    "        self.fc1 = VnUpdateLayer(hidden_size, hidden_size,CN2VN_mask_VNUpdate) # VN update (i=odd)\n",
    "        self.fc2 = CnUpdateLayer(hidden_size, hidden_size,VN2CN_mask_CNUpdate,first=False) # CN update (i=even)\n",
    "        self.fc3 = VnUpdateLayer(hidden_size, hidden_size,CN2VN_mask_VNUpdate) # VN update (i=odd)\n",
    "        self.fc4 = CnUpdateLayer(hidden_size, hidden_size,VN2CN_mask_CNUpdate,first=False) # CN update (i=even)\n",
    "        self.fc5 = VnUpdateLayer(hidden_size, output_size,CN2VN_mask_output) # output \n",
    "        self.bias_matrix = dot_bias_matrix\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # first layer of input to CN update\n",
    "        Channel_LLR = x.clone()\n",
    "        x = 2*torch.atanh(self.fc0(self.tanh(0.5*x)))  # input to CNUpdate (i=1)\n",
    "        x = self.tanh(0.5*(self.fc1(x)+torch.matmul(Channel_LLR,self.bias_matrix))) # CN->VN VNUpdate (i=2)\n",
    "        x = 2*torch.atanh(self.fc2(x)) # VN->CN CNUpdate (i=3)\n",
    "        x = self.tanh(0.5*(self.fc3(x)+torch.matmul(Channel_LLR,self.bias_matrix))) # CN->VN VNUpdate (i=4)\n",
    "        x = 2*torch.atanh(self.fc4(x)) # VN->CN CNUpdate (i=5)\n",
    "        x = self.sigmoid(self.fc5(x)+Channel_LLR) #(i=6)\n",
    "        return x\n",
    "\n",
    "    def apply_sparse_masks(self):\n",
    "        self.fc1.apply_sparse_mask()\n",
    "\n",
    "\n",
    "\n",
    "# create model\n",
    "model = SparseBPNeuralNetwork(input_size,hidden_layer_node,output_size,first_layer_mask,CN2VN_mask_VNUpdate,VN2CN_mask_CNUpdate,CN2VN_mask_output,dot_bias_matrix)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "# 打印初始的权重\n",
    "print(\"Initial weights of fc0:\", model.fc0.weight)\n",
    "print(\"Initial weights of fc1:\", model.fc1.weight)\n",
    "print(\"Initial weights of fc2:\", model.fc2.weight)\n",
    "print(\"Initial weights of fc3:\", model.fc3.weight)\n",
    "print(\"Initial weights of fc4:\", model.fc4.weight)\n",
    "print(\"Initial weights of fc5:\", model.fc5.weight)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fff7d875-3470-4ba7-9fdc-0f2e27b820ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9708, 0.9725, 0.7909, 0.9814, 0.9904, 0.6987, 0.9523, 0.8995, 0.5746,\n",
      "         0.9698, 0.4754, 0.5927, 0.5096, 0.9606, 0.5802]],\n",
      "       grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 生成一些训练数据\n",
    "inputs = torch.tensor([[3.50427,3.56523,1.33009,3.96473,4.633,0.843421,2.99427,2.19127,0.299632,3.46877,-0.119053,0.38125,0.028388,3.19273,0.329816]])\n",
    "targets = torch.zeros(1, output_size)\n",
    "bias_input_hidden = torch.matmul(inputs,dot_bias_matrix)\n",
    "bias_input_output = inputs.clone()\n",
    "# 定义损失函数和优化器  \n",
    "outputs=model(inputs)\n",
    "print(outputs)\n",
    "# train model\n",
    "# for epoch in range(100):\n",
    "#     optimizer.zero_grad()       # clear grad\n",
    "#     outputs = model(inputs)     # forward\n",
    "#     loss = criterion(outputs, targets)  # count loss\n",
    "#     loss.backward()             # backward\n",
    "#     model.apply_sparse_masks()  # 应用稀疏掩码，防止零权重更新\n",
    "#     optimizer.step()            # update parameter\n",
    "\n",
    "# print weights of fcs\n",
    "# print(\"Trained weights of fc1:\", model.fc1.weight.data)\n",
    "# print(\"Trained weights of fc2:\", model.fc2.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "59a8b44a-2acb-4e7e-9fd1-ddd151e6e6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial bias: tensor([-0.4180])\n",
      "Initial weight: tensor([[ 0.0000, -0.1029, -0.3687]])\n",
      "tensor([[ 1.2555, -1.7822,  0.7545],\n",
      "        [ 1.1652,  0.3808, -0.7641],\n",
      "        [ 0.5763,  0.0506, -1.3033],\n",
      "        [-0.3080,  0.3904, -1.0683],\n",
      "        [ 0.8905,  1.1731,  1.9236],\n",
      "        [ 2.9778,  1.4531,  0.3056],\n",
      "        [ 0.2717,  1.1332,  1.2300],\n",
      "        [-0.4211,  0.0416,  1.1718],\n",
      "        [-1.0794, -0.9284, -0.3353],\n",
      "        [-0.9748, -1.3899, -1.3581]])\n",
      "tensor([[ 2.7021],\n",
      "        [-0.6055],\n",
      "        [-1.7897],\n",
      "        [ 0.2842],\n",
      "        [-0.5957],\n",
      "        [ 0.0611],\n",
      "        [ 0.6374],\n",
      "        [ 0.9967],\n",
      "        [ 1.4977],\n",
      "        [-0.4564]])\n",
      "Trained bias: tensor([0.1559])\n",
      "Trained weight: tensor([[ 0.0773, -0.6221,  0.4980]])\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "\n",
    "# # 定义一个简单的线性模型\n",
    "# class SimpleModel(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(SimpleModel, self).__init__()\n",
    "#         self.fc = nn.Linear(3, 1)\n",
    "#         self.fc.weight.data.view(-1)[0]=0\n",
    "#     def forward(self, x):\n",
    "#         return self.fc(x)\n",
    "\n",
    "# # 创建模型实例\n",
    "# model = SimpleModel()\n",
    "\n",
    "# # 打印初始的偏置\n",
    "# print(\"Initial bias:\", model.fc.bias.data)\n",
    "# print(\"Initial weight:\", model.fc.weight.data)\n",
    "# # 定义损失函数和优化器\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# # 生成一些训练数据\n",
    "# inputs = torch.randn(10, 3)\n",
    "# targets = torch.randn(10, 1)\n",
    "# print(inputs)\n",
    "# print(targets)\n",
    "# # 训练模型\n",
    "# for epoch in range(100):\n",
    "#     optimizer.zero_grad()       # 清零梯度\n",
    "#     outputs = model(inputs)     # 前向传播\n",
    "#     loss = criterion(outputs, targets)  # 计算损失\n",
    "#     loss.backward()             # 反向传播\n",
    "#     optimizer.step()            # 更新参数\n",
    "\n",
    "# # 打印训练后的偏置\n",
    "# print(\"Trained bias:\", model.fc.bias.data)\n",
    "# print(\"Trained weight:\", model.fc.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "cab514ac-e15b-4cd4-acb3-b9ada678fc86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],\n",
      "        [0.0000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n",
      "tensor([[1.0000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],\n",
      "        [1.0000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],\n",
      "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]])\n",
      "tensor([[0.0000, 0.0000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n",
      "tensor([[1.0000, 1.0000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],\n",
      "        [1.0000, 1.0000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],\n",
      "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]])\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.5000, 0.5000, 0.5000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.5000, 0.5000, 0.5000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n",
      "tensor([[1.0000, 1.0000, 1.0000, 0.5000, 0.5000, 0.5000, 0.5000],\n",
      "        [1.0000, 1.0000, 1.0000, 0.5000, 0.5000, 0.5000, 0.5000],\n",
      "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]])\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.5000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.5000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n",
      "tensor([[1.0000, 1.0000, 1.0000, 1.0000, 0.5000, 0.5000, 0.5000],\n",
      "        [1.0000, 1.0000, 1.0000, 1.0000, 0.5000, 0.5000, 0.5000],\n",
      "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]])\n",
      "tensor([[0.5000, 0.0000, 0.0000, 0.0000, 0.5000, 0.5000, 0.5000],\n",
      "        [0.5000, 0.0000, 0.0000, 0.0000, 0.5000, 0.5000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n",
      "tensor([[0.5000, 1.0000, 1.0000, 1.0000, 0.5000, 0.5000, 0.5000],\n",
      "        [0.5000, 1.0000, 1.0000, 1.0000, 0.5000, 0.5000, 0.5000],\n",
      "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]])\n",
      "Input:\n",
      " tensor([[0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],\n",
      "        [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n",
      "Output:\n",
      " tensor([[0.0313, 0.0625, 0.1252, 0.2513, 0.1252],\n",
      "        [0.0313, 0.0625, 0.1252, 0.2513, 0.1252],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class CnUpdateLayer(nn.Module):\n",
    "#     def __init__(self, in_features, out_features, layer_mask):\n",
    "#         super(CnUpdateLayer, self).__init__()\n",
    "#         self.in_features = in_features\n",
    "#         self.out_features = out_features\n",
    "#         self.layer_mask = torch.tensor(layer_mask, dtype=torch.float32)  # Convert to tensor\n",
    "#         self.reset_parameters()\n",
    "    \n",
    "#     def reset_parameters(self):\n",
    "#         # 这里可以初始化其他参数，如果有的话\n",
    "#         pass\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         # 应用掩码并处理每个输出神经元\n",
    "#         batch_size = x.size(0)\n",
    "#         output = torch.zeros(batch_size, self.out_features, device=x.device)\n",
    "\n",
    "#         for i in range(self.out_features):\n",
    "#             # 应用每个输出神经元的掩码\n",
    "#             mask = self.layer_mask[i]\n",
    "#             x_masked = x * mask  # 掩码应用于输入数据\n",
    "#             print(x_masked)\n",
    "#             # 将零值替换为一，以避免影响乘积\n",
    "#             x_nonzero = torch.where(x_masked != 0, x_masked, torch.ones_like(x_masked))\n",
    "#             print(x_nonzero)\n",
    "#             # 计算掩码后数据的乘积\n",
    "#             product = torch.prod(x_nonzero, dim=1)\n",
    "            \n",
    "#             # 检查全零行，将这些行的乘积结果置为1，避免影响后续的双曲正切反函数\n",
    "#             non_zero_prod = torch.where((x_masked != 0).any(dim=1), product, torch.zeros_like(product))\n",
    "#             # print(non_zero_prod)\n",
    "#             # 计算双曲正切反函数并乘以2\n",
    "#             output[:, i] = 2 * torch.atanh(non_zero_prod)\n",
    "        \n",
    "#         return output\n",
    "\n",
    "# # 定义掩码\n",
    "# layer_mask = [\n",
    "#     [0, 1, 1, 1, 1,1,1],\n",
    "#     [0, 0, 1, 1, 1,1,1],\n",
    "#     [0, 0, 0, 1, 1,1,1],\n",
    "#     [0, 0, 0, 0, 1,1,1],\n",
    "#     [1, 0, 0, 0, 1,1,1]\n",
    "# ]\n",
    "\n",
    "# # 创建层实例\n",
    "# in_features = 7\n",
    "# out_features = 5\n",
    "# layer = CnUpdateLayer(in_features, out_features, layer_mask)\n",
    "\n",
    "# # 随机生成一些输入数据，假设有一个batch size为3\n",
    "# x_input = torch.ones(3, in_features)\n",
    "# x_input = torch.tensor([[0.5,0.5,0.5,0.5,0.5,0.5,0.5],\n",
    "#                         [0.5,0.5,0.5,0.5,0.5,0.5,0.5],\n",
    "#                         [0,0,0,0,0,0,0]])\n",
    "# # 通过层传递数据\n",
    "# output = layer(x_input)\n",
    "\n",
    "# print(\"Input:\\n\", x_input)\n",
    "# print(\"Output:\\n\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977b3b22-71f2-446e-9ba4-a15e7069aadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_mask=torch.tensor([0,1,1,1,1])\n",
    "# non_zero_prod = torch.where((x_masked != 0).any(dim=1), product, torch.zeros_like(product))\n",
    "# print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
